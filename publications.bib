@article{sheng2024lightweight,
  title={A lightweight hybrid model with location-preserving vit for efficient food recognition},
  author={Sheng, Guorui and Min, Weiqing and Zhu, Xiangyi and Xu, Liang and Sun, Qingshuo and Yang, Yancun and Wang, Lili and Jiang, Shuqiang},
  journal={Nutrients},
  volume={16},
  number={2},
  pages={200},
  year={2024},
  publisher={MDPI}
}

@article{sheng2022food,
  title={Food recognition via an efficient neural network with transformer grouping},
  author={Sheng, Guorui and Sun, Shuqi and Liu, Chengxu and Yang, Yancun},
  journal={International Journal of Intelligent Systems},
  volume={37},
  number={12},
  pages={11465--11481},
  year={2022},
  publisher={Wiley Online Library}
}

@article{sheng2024lightweight1,
  title={Lightweight Food Image Recognition With Global Shuffle Convolution},
  author={Sheng, Guorui and Min, Weiqing and Yao, Tao and Song, Jingru and Yang, Yancun and Wang, Lili and Jiang, Shuqiang},
  journal={IEEE Transactions on AgriFood Electronics},
  year={2024},
  publisher={IEEE}
}

@article{yang2024lightweight,
  title={Lightweight Food Recognition via Aggregation Block and Feature Encoding},
  author={Yang, Yancun and Min, Weiqing and Song, Jingru and Sheng, Guorui and Wang, Lili and Jiang, Shuqiang},
  journal={ACM Transactions on Multimedia Computing, Communications and Applications},
  year={2024},
  publisher={ACM New York, NY}
}

@article{ SPKX20240306011,
  author = { 曹品丹 and  闵巍庆 and  宋佳骏 and  盛国瑞 and  杨延村 and  王丽丽 and  蒋树强 },
  title = {基于增强Vision Transformer的哈希食品图像检索},
  journal = {食品科学},
  pages = {1-12},
  issn = {1002-6630},
}

@article{ SPKJ2024052800G,
  author = { 宋静茹 and  闵巍庆 and  周鹏飞 and  饶全瑞 and  盛国瑞 and  杨延村 and  王丽丽 and  蒋树强 },
  title = {基于Transformer的零样本食品图像检测},
  journal = {食品工业科技},
  pages = {1-15},
  issn = {1002-0306},
  doi = {10.13386/j.issn1002-0306.2024030027}
}

@article{LIU2025128636,
title = {Channel Grouping Vision Transformer for Lightweight Fruit and Vegetable Recognition},
journal = {Expert Systems with Applications},
pages = {128636},
year = {2025},
issn = {0957-4174},
doi = {10.1016/j.eswa.2025.128636},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425022559},
author = {Chengxu Liu and Weiqing Min and Jingru Song and Yancun Yang and Guorui Sheng and Tao Yao and Lili Wang and Shuqiang Jiang},
keywords = {Fruit Recognition, Vegetable Recognition, Lightweight, Deep Learning, Computer Vision},
abstract = {Recognizing fruit and vegetable is crucial for improving processing efficiency, automating harvesting, and facilitating dietary nutrition management. The diverse applications of fruit and vegetable recognition require deployment on end devices with limited resources, such as memory and computing power. The key challenge lies in designing lightweight recognition algorithms. However, current lightweight methods still rely on simple CNN-based networks, which fail to deeply explore and specifically analyze the unique features of fruit and vegetable images, resulting in unsatisfactory recognition performance. To address this challenge, we propose a novel lightweight recognition network termed Channel Grouping Vision Transformer (CGViT). CGViT utilizes a channel grouping mechanism and half-convolution to enhance feature extraction capability while reducing complexity. This design enables the model to capture three discriminative types of features from images. Subsequently, the Transformer is employed for feature fusion and global information extraction, ultimately creating an efficient neural network model for fruit and vegetable recognition. The proposed CGViT approach achieved recognition accuracies of 71.26%, 99.99%, 98.92%, and 61.33% on four fruit and vegetable datasets, respectively, outperforming state-of-the-art methods (MobileViTV2, MixNet, MobileNetV2). The maximum memory usage during training is only 6.48GB, which is merely 13.8% of that required by state-of-the-art methods(MobileViTv2). The fruit and vegetable recognition model proposed in this study offers a more profound and effective solution, providing valuable insights for future research and practical applications in this domain. The code is available at https://github.com/Axboexx/CGViT.}
}

@article{zhang_cross-layer_nodate,
	title = {Cross-layer and selective distillation for asymmetric image retrieval},
	abstract = {Existing asymmetric retrieval methods primarily rely on aligning global features to transfer semantic information. However, they often struggle to convey knowledge effectively across different network layers, limiting fine-grained alignment in feature representation spaces. To address this limitation, we propose a Cross-Layer and Selective Distillation (CLSD) framework. It first introduces a semantic-aware cross-layer feature distillation mechanism, where an attention-guided soft layer alignment strategy enables the student model to dynamically select and integrate the most relevant semantic knowledge from multiple intermediate teacher layers, based on its own layer’s semantic requirements. This alleviates the knowledge transfer challenges arising from architectural asymmetry. Furthermore, considering the importance of ranking consistency in fine-grained food image retrieval, we propose a decoupled differential relation distillation approach based on unambiguous samples. This method emphasizes the teacher model’s discriminative power and ranking behavior on unambiguous samples, while filtering out noisy signals from ambiguous ones. As a result, the student learns more reliable relative relationships between samples, ensuring consistency in ranking order between query and gallery features. Extensive experiments on four benchmark datasets demonstrate that our method consistently surpasses existing state-of-the-art techniques, highlighting its effectiveness in asymmetric fine-grained retrieval tasks.},
	language = {en},
	author = {Zhang, Shijie and Min, Weiqing and Yao, Fangyuan and Sheng, Guorui and Jiang, Shuqiang},
}

@article{zhu2025,
	title = {DPFA-Net: A Lightweight Hybrid Neural Network with Dual Path Feature Aggregation for Food Image Recognition},
	abstract = {Food image recognition holds significant application potential in the field of computer vision. However, due to performance constraints on mobile devices, the scale and computational overhead of models face notable limitations, making effective deployment on mobile platforms challenging. To address this issue, this paper proposes a lightweight Dual-Path Feature Aggregation Network (DPFA-Net), designed to enhance the performance of food recognition tasks through efficient local and global feature extraction strategies. Specifically, the DPFA architecture comprises two core modules: the GhostBottleneck module for local feature encoding and the Position Mamba Vision Transformer (PM-ViT) module for global modeling. In this work, the GhostBottleneck module is utilized to extract local features from images. Furthermore, by integrating the Mamba structure with the Separable Self-Attention (SSA) structure, we construct the Mamba Attention (MA) module, which replaces the traditional Attention mechanism in Vision Transformers to build the PM-ViT module, enabling the capture of global features in food images. The redesigned DPFA-Net effectively fuses local and global information, achieving efficient food image recognition. Experimental results demonstrate that, on the ETHZ Food-101, Vireo Food-172, and UEC Food-256 benchmark datasets, DPFA-Net achieves Top-1 accuracies of 91.46%, 91.59%, and 75.33%, respectively, while reducing approximately 8.4M parameters. Compared to MobileViTv2, DPFA-Net improves performance by 1.5%-3.9%, fully validating the effectiveness and superiority of the DPFA architecture.},
	language = {en},
	author = {Xiangyi Zhu and Wenli Zhang and Yingnan Sheng and Congrui Lv and Guorui Sheng and Weiqing Min and Shuqiang Jiang},
}

@article{wang2025llm,
  title={LLM-Informed Global-Local Contextualization for Zero-Shot Food Detection},
  author={Wang, Xinlong and Min, Weiqing and Sheng, Guorui and Song, Jingru and Yang, Yancun and Yao, Tao and Jiang, Shuqiang},
  journal={Pattern Recognition},
  pages={112928},
  year={2025},
  publisher={Elsevier}
}

@article{Zhu2026,
  author    = {Zhu, Xiangyi and Zhang, Wenli and Sheng, Yingnan and Lv, Congrui and Sheng, Guorui and Min, Weiqing and Jiang, Shuqiang},
  title     = {DPFA-net: a lightweight hybrid neural network with dual path feature aggregation for food image recognition},
  journal   = {Multimedia Systems},
  year      = {2026},
  volume    = {32},
  number    = {2},
  pages     = {80},
  issn      = {1432-1882},
  doi       = {10.1007/s00530-025-02143-3},
  url       = {https://doi.org/10.1007/s00530-025-02143-3},
  abstract  = {Food image recognition holds significant application potential in the field of computer vision. However, due to performance constraints on mobile devices, the scale and computational overhead of models face notable limitations, making effective deployment on mobile platforms challenging. To address this issue, this paper proposes a lightweight Dual-Path Feature Aggregation Network (DPFA-Net), designed to enhance the performance of food recognition tasks through efficient local and global feature extraction strategies. Specifically, the DPFA architecture comprises two core modules: the GhostBottleneck module for local feature encoding and the Position Mamba Vision Transformer (PM-ViT) module for global modeling. In this work, the GhostBottleneck module is utilized to extract local features from images. Furthermore, by integrating the Mamba structure with the Separable Self-Attention (SSA) structure, we construct the Mamba Attention (MA) module, which replaces the traditional Attention mechanism in Vision Transformers to build the PM-ViT module, enabling the capture of global features in food images. The redesigned DPFA-Net effectively fuses local and global information, achieving efficient food image recognition. The experiments were conducted on the ETHZ Food-101, Vireo Food-172, and UEC Food-256 datasets. The results show that, while reducing the number of parameters, DPFA-Net achieved Top-1 accuracies of 91.46%, 91.59%, and 75.33%, respectively, representing a 1.50%–3.9% improvement over MobileViTv2. Compared to MobileViTv2, DPFA-Net improves performance by 1.50–3.9%, fully validating the effectiveness and superiority of the DPFA architecture.}
}