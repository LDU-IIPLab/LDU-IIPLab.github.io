@article{sheng2024lightweight,
  title={A lightweight hybrid model with location-preserving vit for efficient food recognition},
  author={Sheng, Guorui and Min, Weiqing and Zhu, Xiangyi and Xu, Liang and Sun, Qingshuo and Yang, Yancun and Wang, Lili and Jiang, Shuqiang},
  journal={Nutrients},
  volume={16},
  number={2},
  pages={200},
  year={2024},
  publisher={MDPI}
}

@article{sheng2022food,
  title={Food recognition via an efficient neural network with transformer grouping},
  author={Sheng, Guorui and Sun, Shuqi and Liu, Chengxu and Yang, Yancun},
  journal={International Journal of Intelligent Systems},
  volume={37},
  number={12},
  pages={11465--11481},
  year={2022},
  publisher={Wiley Online Library}
}

@article{sheng2024lightweight1,
  title={Lightweight Food Image Recognition With Global Shuffle Convolution},
  author={Sheng, Guorui and Min, Weiqing and Yao, Tao and Song, Jingru and Yang, Yancun and Wang, Lili and Jiang, Shuqiang},
  journal={IEEE Transactions on AgriFood Electronics},
  year={2024},
  publisher={IEEE}
}

@article{yang2024lightweight,
  title={Lightweight Food Recognition via Aggregation Block and Feature Encoding},
  author={Yang, Yancun and Min, Weiqing and Song, Jingru and Sheng, Guorui and Wang, Lili and Jiang, Shuqiang},
  journal={ACM Transactions on Multimedia Computing, Communications and Applications},
  year={2024},
  publisher={ACM New York, NY}
}

@article{ SPKX20240306011,
  author = { 曹品丹 and  闵巍庆 and  宋佳骏 and  盛国瑞 and  杨延村 and  王丽丽 and  蒋树强 },
  title = {基于增强Vision Transformer的哈希食品图像检索},
  journal = {食品科学},
  pages = {1-12},
  issn = {1002-6630},
}

@article{ SPKJ2024052800G,
  author = { 宋静茹 and  闵巍庆 and  周鹏飞 and  饶全瑞 and  盛国瑞 and  杨延村 and  王丽丽 and  蒋树强 },
  title = {基于Transformer的零样本食品图像检测},
  journal = {食品工业科技},
  pages = {1-15},
  issn = {1002-0306},
  doi = {10.13386/j.issn1002-0306.2024030027}
}

@article{LIU2025128636,
title = {Channel Grouping Vision Transformer for Lightweight Fruit and Vegetable Recognition},
journal = {Expert Systems with Applications},
pages = {128636},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.128636},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425022559},
author = {Chengxu Liu and Weiqing Min and Jingru Song and Yancun Yang and Guorui Sheng and Tao Yao and Lili Wang and Shuqiang Jiang},
keywords = {Fruit Recognition, Vegetable Recognition, Lightweight, Deep Learning, Computer Vision},
abstract = {Recognizing fruit and vegetable is crucial for improving processing efficiency, automating harvesting, and facilitating dietary nutrition management. The diverse applications of fruit and vegetable recognition require deployment on end devices with limited resources, such as memory and computing power. The key challenge lies in designing lightweight recognition algorithms. However, current lightweight methods still rely on simple CNN-based networks, which fail to deeply explore and specifically analyze the unique features of fruit and vegetable images, resulting in unsatisfactory recognition performance. To address this challenge, we propose a novel lightweight recognition network termed Channel Grouping Vision Transformer (CGViT). CGViT utilizes a channel grouping mechanism and half-convolution to enhance feature extraction capability while reducing complexity. This design enables the model to capture three discriminative types of features from images. Subsequently, the Transformer is employed for feature fusion and global information extraction, ultimately creating an efficient neural network model for fruit and vegetable recognition. The proposed CGViT approach achieved recognition accuracies of 71.26%, 99.99%, 98.92%, and 61.33% on four fruit and vegetable datasets, respectively, outperforming state-of-the-art methods (MobileViTV2, MixNet, MobileNetV2). The maximum memory usage during training is only 6.48GB, which is merely 13.8% of that required by state-of-the-art methods(MobileViTv2). The fruit and vegetable recognition model proposed in this study offers a more profound and effective solution, providing valuable insights for future research and practical applications in this domain. The code is available at https://github.com/Axboexx/CGViT.}
}

@article{zhang_cross-layer_nodate,
	title = {Cross-layer and selective distillation for asymmetric image retrieval},
	abstract = {Existing asymmetric retrieval methods primarily rely on aligning global features to transfer semantic information. However, they often struggle to convey knowledge effectively across different network layers, limiting fine-grained alignment in feature representation spaces. To address this limitation, we propose a Cross-Layer and Selective Distillation (CLSD) framework. It first introduces a semantic-aware cross-layer feature distillation mechanism, where an attention-guided soft layer alignment strategy enables the student model to dynamically select and integrate the most relevant semantic knowledge from multiple intermediate teacher layers, based on its own layer’s semantic requirements. This alleviates the knowledge transfer challenges arising from architectural asymmetry. Furthermore, considering the importance of ranking consistency in fine-grained food image retrieval, we propose a decoupled differential relation distillation approach based on unambiguous samples. This method emphasizes the teacher model’s discriminative power and ranking behavior on unambiguous samples, while filtering out noisy signals from ambiguous ones. As a result, the student learns more reliable relative relationships between samples, ensuring consistency in ranking order between query and gallery features. Extensive experiments on four benchmark datasets demonstrate that our method consistently surpasses existing state-of-the-art techniques, highlighting its effectiveness in asymmetric fine-grained retrieval tasks.},
	language = {en},
	author = {Zhang, Shijie and Min, Weiqing and Yao, Fangyuan and Sheng, Guorui and Jiang, Shuqiang},
}