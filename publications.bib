@article{sheng2024lightweight,
  title={A lightweight hybrid model with location-preserving vit for efficient food recognition},
  author={Sheng, Guorui and Min, Weiqing and Zhu, Xiangyi and Xu, Liang and Sun, Qingshuo and Yang, Yancun and Wang, Lili and Jiang, Shuqiang},
  journal={Nutrients},
  volume={16},
  number={2},
  pages={200},
  year={2024},
  publisher={MDPI}
}

@article{sheng2022food,
  title={Food recognition via an efficient neural network with transformer grouping},
  author={Sheng, Guorui and Sun, Shuqi and Liu, Chengxu and Yang, Yancun},
  journal={International Journal of Intelligent Systems},
  volume={37},
  number={12},
  pages={11465--11481},
  year={2022},
  publisher={Wiley Online Library}
}

@article{sheng2024lightweight1,
  title={Lightweight Food Image Recognition With Global Shuffle Convolution},
  author={Sheng, Guorui and Min, Weiqing and Yao, Tao and Song, Jingru and Yang, Yancun and Wang, Lili and Jiang, Shuqiang},
  journal={IEEE Transactions on AgriFood Electronics},
  year={2024},
  publisher={IEEE}
}

@article{yang2024lightweight,
  title={Lightweight Food Recognition via Aggregation Block and Feature Encoding},
  author={Yang, Yancun and Min, Weiqing and Song, Jingru and Sheng, Guorui and Wang, Lili and Jiang, Shuqiang},
  journal={ACM Transactions on Multimedia Computing, Communications and Applications},
  year={2024},
  publisher={ACM New York, NY}
}

@article{ SPKX20240306011,
  author = { 曹品丹 and  闵巍庆 and  宋佳骏 and  盛国瑞 and  杨延村 and  王丽丽 and  蒋树强 },
  title = {基于增强Vision Transformer的哈希食品图像检索},
  journal = {食品科学},
  pages = {1-12},
  issn = {1002-6630},
}

@article{ SPKJ2024052800G,
  author = { 宋静茹 and  闵巍庆 and  周鹏飞 and  饶全瑞 and  盛国瑞 and  杨延村 and  王丽丽 and  蒋树强 },
  title = {基于Transformer的零样本食品图像检测},
  journal = {食品工业科技},
  pages = {1-15},
  issn = {1002-0306},
  doi = {10.13386/j.issn1002-0306.2024030027}
}

@article{LIU2025128636,
title = {Channel Grouping Vision Transformer for Lightweight Fruit and Vegetable Recognition},
journal = {Expert Systems with Applications},
pages = {128636},
year = {2025},
issn = {0957-4174},
doi = {10.1016/j.eswa.2025.128636},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425022559},
author = {Chengxu Liu and Weiqing Min and Jingru Song and Yancun Yang and Guorui Sheng and Tao Yao and Lili Wang and Shuqiang Jiang},
keywords = {Fruit Recognition, Vegetable Recognition, Lightweight, Deep Learning, Computer Vision},
abstract = {Recognizing fruit and vegetable is crucial for improving processing efficiency, automating harvesting, and facilitating dietary nutrition management. The diverse applications of fruit and vegetable recognition require deployment on end devices with limited resources, such as memory and computing power. The key challenge lies in designing lightweight recognition algorithms. However, current lightweight methods still rely on simple CNN-based networks, which fail to deeply explore and specifically analyze the unique features of fruit and vegetable images, resulting in unsatisfactory recognition performance. To address this challenge, we propose a novel lightweight recognition network termed Channel Grouping Vision Transformer (CGViT). CGViT utilizes a channel grouping mechanism and half-convolution to enhance feature extraction capability while reducing complexity. This design enables the model to capture three discriminative types of features from images. Subsequently, the Transformer is employed for feature fusion and global information extraction, ultimately creating an efficient neural network model for fruit and vegetable recognition. The proposed CGViT approach achieved recognition accuracies of 71.26%, 99.99%, 98.92%, and 61.33% on four fruit and vegetable datasets, respectively, outperforming state-of-the-art methods (MobileViTV2, MixNet, MobileNetV2). The maximum memory usage during training is only 6.48GB, which is merely 13.8% of that required by state-of-the-art methods(MobileViTv2). The fruit and vegetable recognition model proposed in this study offers a more profound and effective solution, providing valuable insights for future research and practical applications in this domain. The code is available at https://github.com/Axboexx/CGViT.}
}

@article{wang2025llm,
  title={LLM-Informed Global-Local Contextualization for Zero-Shot Food Detection},
  author={Wang, Xinlong and Min, Weiqing and Sheng, Guorui and Song, Jingru and Yang, Yancun and Yao, Tao and Jiang, Shuqiang},
  journal={Pattern Recognition},
  pages={112928},
  year={2025},
  publisher={Elsevier}
}

@article{Zhu2026,
  author    = {Zhu, Xiangyi and Zhang, Wenli and Sheng, Yingnan and Lv, Congrui and Sheng, Guorui and Min, Weiqing and Jiang, Shuqiang},
  title     = {DPFA-net: a lightweight hybrid neural network with dual path feature aggregation for food image recognition},
  journal   = {Multimedia Systems},
  year      = {2026},
  volume    = {32},
  number    = {2},
  pages     = {80},
  issn      = {1432-1882},
  doi       = {10.1007/s00530-025-02143-3},
  url       = {https://doi.org/10.1007/s00530-025-02143-3},
  abstract  = {Food image recognition holds significant application potential in the field of computer vision. However, due to performance constraints on mobile devices, the scale and computational overhead of models face notable limitations, making effective deployment on mobile platforms challenging. To address this issue, this paper proposes a lightweight Dual-Path Feature Aggregation Network (DPFA-Net), designed to enhance the performance of food recognition tasks through efficient local and global feature extraction strategies. Specifically, the DPFA architecture comprises two core modules: the GhostBottleneck module for local feature encoding and the Position Mamba Vision Transformer (PM-ViT) module for global modeling. In this work, the GhostBottleneck module is utilized to extract local features from images. Furthermore, by integrating the Mamba structure with the Separable Self-Attention (SSA) structure, we construct the Mamba Attention (MA) module, which replaces the traditional Attention mechanism in Vision Transformers to build the PM-ViT module, enabling the capture of global features in food images. The redesigned DPFA-Net effectively fuses local and global information, achieving efficient food image recognition. The experiments were conducted on the ETHZ Food-101, Vireo Food-172, and UEC Food-256 datasets. The results show that, while reducing the number of parameters, DPFA-Net achieved Top-1 accuracies of 91.46%, 91.59%, and 75.33%, respectively, representing a 1.50%–3.9% improvement over MobileViTv2. Compared to MobileViTv2, DPFA-Net improves performance by 1.50–3.9%, fully validating the effectiveness and superiority of the DPFA architecture.}
}

@InProceedings{10.1007/978-981-95-5761-5_12,
author="Zhang, Shijie
and Min, Weiqing
and Yao, Fangyuan
and Sheng, Guorui
and Jiang, Shuqiang",
editor="Kittler, Josef
and Xiong, Hongkai
and Yang, Jian
and Chen, Xilin
and Lu, Jiwen
and Lin, Weiyao
and Yu, Jingyi
and Zheng, Weishi",
title="Cross-Layer and Selective Distillation for Asymmetric Image Retrieval",
booktitle="Pattern Recognition and Computer Vision",
year="2026",
publisher="Springer Nature Singapore",
address="Singapore",
pages="163--177",
abstract="Existing asymmetric retrieval methods primarily rely on aligning global features to transfer semantic information. However, they often struggle to convey knowledge effectively across different network layers, limiting fine-grained alignment in feature representation spaces. To address this limitation, we propose a Cross-Layer and Selective Distillation (CLSD) framework. It first introduces a semantic-aware cross-layer feature distillation mechanism, where an attention-guided soft layer alignment strategy enables the student model to dynamically select and integrate the most relevant semantic knowledge from multiple intermediate teacher layers, based on its own layer's semantic requirements. This alleviates the knowledge transfer challenges arising from architectural asymmetry. Furthermore, considering the importance of ranking consistency in fine-grained food image retrieval, we propose a decoupled differential relation distillation approach based on unambiguous samples. This method emphasizes the teacher model's discriminative power and ranking behavior on unambiguous samples, while filtering out noisy signals from ambiguous ones. As a result, the student learns more reliable relative relationships between samples, ensuring consistency in ranking order between query and gallery features. Extensive experiments on four benchmark datasets demonstrate that our method consistently surpasses existing state-of-the-art techniques, highlighting its effectiveness in asymmetric fine-grained retrieval tasks.",
isbn="978-981-95-5761-5"
}

@article{cao-2025-foodhash,
author = {Cao, Pindan and Min, Weiqing and Sheng, Guorui and Song, Yongqiang and Yao, Tao and Wang, Lili and Jiang, Shuqiang},
title = {FoodHash: Context-Aware Proxy Interaction and Fusion for Food Image Retrieval},
year = {2026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
doi = {10.1145/3787854},
abstract = {Vision-based food image retrieval has garnered significant attention due to its potential for critical applications in dietary and health management. However, food images exhibit more complex feature distributions and lack the geometric regularity and structured patterns typically observed in general image retrieval tasks. This complexity poses a challenge for existing models to extract fine-grained features and semantic information, thereby compromising retrieval performance. To address this challenge, we propose FoodHash, a context-aware proxy interaction and fusion hashing method for food image retrieval. The method incorporates an Aggregation-Interaction-Propagation (AIP) module that facilitates contextual information exchange among patch tokens within the same feature map, guided by proxy tokens, thereby effectively capturing the intricate details of food images. Furthermore, to leverage the rich semantic information in food images, a Cross-Fusion Module is introduced to efficiently integrate multi-scale information and enhance feature representation. Additionally, we employ a novel loss function to optimize hash learning by ensuring consistency between hash codes and the semantic space, thereby enhancing the learning capability of hash coding. Extensive experiments on three publicly available food datasets demonstrate that FoodHash significantly surpasses existing models in retrieval performance. Specifically, on the ETH Food-101 dataset, FoodHash achieves improvements of 18.1\%, 6.7\%, 5.2\% and 4.5\% over the suboptimal method PTLCH for 16-bits, 32-bits, 48-bits and 64-bits hash codes, respectively. The source code will be made publicly available upon publication of the paper.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = feb,
keywords = {Food image retrieval, Food computing, Hash retrieval, Deep hashing learning}
}


@Article{foods15030534,
AUTHOR = {Wang, Shenglong and Sheng, Guorui},
TITLE = {Hybrid Decoding with Co-Occurrence Awareness for Fine-Grained Food Image Segmentation},
JOURNAL = {Foods},
VOLUME = {15},
YEAR = {2026},
NUMBER = {3},
ARTICLE-NUMBER = {534},
URL = {https://www.mdpi.com/2304-8158/15/3/534},
ISSN = {2304-8158},
ABSTRACT = {Fine-grained food image segmentation is essential for accurate dietary assessment and nutritional analysis, yet remains highly challenging due to ambiguous boundaries, inter-class similarity, and dense layouts of meals containing many different ingredients in real-world settings. Existing methods based solely on CNNs, Transformers, or Mamba architectures often fail to simultaneously preserve fine-grained local details and capture contextual dependencies over long distances. To address these limitations, we propose HDF (Hybrid Decoder for Food Image Segmentation), a novel decoding framework built upon the MambaVision backbone. Our approach first employs a convolution-based feature pyramid network (FPN) to extract multi-stage features from the encoder. These features are then thoroughly fused across scales using a Cross-Layer Mamba module that models inter-level dependencies with linear complexity. Subsequently, an Attention Refinement module integrates global semantic context through spatial–channel reweighting. Finally, a Food Co-occurrence Module explicitly enhances food-specific semantics by learning dynamic co-occurrence patterns among categories, improving segmentation of visually similar or frequently co-occurring ingredients. Evaluated on two widely used, high-quality benchmarks, FoodSeg103 and UEC-FoodPIX Complete, which are standard datasets for fine-grained food segmentation, HDF achieves a 52.25% mean Intersection-over-Union (mIoU) on FoodSeg103 and a 76.16% mIoU on UEC-FoodPIX Complete, outperforming current state-of-the-art methods by a clear margin. These results demonstrate that HDF’s hybrid design and explicit co-occurrence awareness effectively address key challenges in food image segmentation, providing a robust foundation for practical applications in dietary logging, nutritional estimation, and food safety inspection.},
DOI = {10.3390/foods15030534}
}
